{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2332ef98",
   "metadata": {},
   "source": [
    "# Word Embeddings :\n",
    "\n",
    "* Word embeddings are numerical representation of words or phrases that capture the meaning of the words in a vector space. \n",
    "* In word embeddings, each word possessed a **universal numerical representation** associated with its **own definition**.\n",
    "* In this, it is possible to represent each word by a **unique vector independent of the text under consideration**.\n",
    "* They are useful for the natural language processing tasks because they capture the **semantic similarity and semantic distance between words**.\n",
    "\n",
    "* Benefits of word embeddings :\n",
    "    1. They **retain semantic similarity**\n",
    "    2. Word embeddings have **dense vectors**\n",
    "    3. They have a **constant vector size**\n",
    "    4. Their vector representations are **absolute**\n",
    "    5. They have **Multiple Embedding Models**\n",
    "    \n",
    "\n",
    "In this worksheet, we will use **gensim** library to implement *word embeddings*\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bd5b1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: gensim\n",
      "Version: 4.3.2\n",
      "Summary: Python framework for fast Vector Space Modelling\n",
      "Home-page: https://radimrehurek.com/gensim/\n",
      "Author: Radim Rehurek\n",
      "Author-email: me@radimrehurek.com\n",
      "License: LGPL-2.1-only\n",
      "Location: D:\\anaconda\\Lib\\site-packages\n",
      "Requires: numpy, scipy, smart-open\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "# Install the gensim\n",
    "# !pip install --upgrade gensim\n",
    "\n",
    "# Check the version is 4+\n",
    "!pip show gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4dcbfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# Import the libraries and\n",
    "# Load the gensim model : 'glove-wiki-gigaword-50'\n",
    "# Since it takes less time to download and load\n",
    "# Results are roughly similar\n",
    "\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "\n",
    "model = api.load(\"glove-wiki-gigaword-50\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e076a789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usually the 'word2vec-google-news-300' model takes long to download and load.\n",
    "# If you want to work on this, then :\n",
    "# model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44be53ac",
   "metadata": {},
   "source": [
    "Functions in `gensim` : \n",
    "\n",
    "    * model['word'] : Returns the actual word vector\n",
    "    * most_similar('word') : For a list of words that are most similar to a given word.\n",
    "    * similarity(\"word1\", \"word2\") : To compute the cosine similarity between two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab77d61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0076543  0.93456   -0.73189   -0.55162    0.76977    0.35925\n",
      " -1.1365    -1.1632     0.34214    0.29145   -0.8711     0.9197\n",
      " -0.47069   -0.22834    1.4777    -0.81714   -0.17466   -0.51093\n",
      " -0.28354    0.23292    0.71832    0.23414    0.49443    0.35483\n",
      "  0.76889   -1.4374    -1.7457    -0.28994   -0.10156   -0.36959\n",
      "  2.5502    -1.0581    -0.049416  -0.25524   -0.63303    0.02671\n",
      " -0.18733    0.20206   -0.26288   -0.41418    0.83473   -0.14227\n",
      " -0.28125    0.098155  -0.17096    0.52408    0.31851   -0.089847\n",
      " -0.27223   -0.0088736]\n"
     ]
    }
   ],
   "source": [
    "# To get the acutal word vector\n",
    "print(model['book'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d0563a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('blackberry', 0.7543067932128906), ('chips', 0.7438643574714661), ('iphone', 0.7429665327072144), ('microsoft', 0.7334205508232117), ('ipad', 0.7331036329269409), ('pc', 0.7217226624488831), ('ipod', 0.7199784517288208), ('intel', 0.7192243337631226), ('ibm', 0.7146540880203247), ('software', 0.7093585729598999)]\n"
     ]
    }
   ],
   "source": [
    "# To get the similar words for a given words\n",
    "print(model.most_similar(\"apple\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1556e9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('peaches', 0.862353503704071), ('oranges', 0.859447717666626), ('cherries', 0.8461860418319702), ('mangoes', 0.8264982104301453), ('apricots', 0.8242633938789368), ('strawberries', 0.8229067325592041), ('potatoes', 0.8179376125335693), ('melons', 0.7980057597160339), ('berries', 0.7946050763130188), ('vegetables', 0.792052149772644)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(\"apples\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f7cb08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity score between apple and banana: 0.5633737\n",
      "similarity score between apple and dog:    0.41387236\n",
      "similarity score between cat   and dog:    0.92180055\n"
     ]
    }
   ],
   "source": [
    "# To get the similarity score between two words\n",
    "print(\"similarity score between apple and banana:\", model.similarity(\"apples\", \"banana\")) \n",
    "print(\"similarity score between apple and dog:   \", model.similarity(\"apple\", \"dog\")) \n",
    "print(\"similarity score between cat   and dog:   \", model.similarity(\"cat\", \"dog\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471ffcfa",
   "metadata": {},
   "source": [
    "### Vocabulary\n",
    "\n",
    "Let's take a look at the **word2Vec** vocabualry\n",
    "\n",
    "Returns a dictionary where **key** are **words** and **index** are **values** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a224eac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = model.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdb1be7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reaud' 'ex-gay' 'powerless' 'stinchcomb' 'horsehead']\n",
      "['furgal' 'umaria' 'daedalus' 'joulwan' 'mulleavy']\n",
      "['trotti' 'absolves' 'engadin' 'dunai' 'witherell']\n",
      "['cowls' 'futari' 'odgen' 'zapeta' 'neatness']\n",
      "['polypeptides' 'thorndyke' 'ibs' 'karlović' 'nacreous']\n",
      "['unaccompanied' 'impressionist' 'levines' 'astrue' 'ovamboland']\n",
      "['universalistic' 'oast' '77' 'doorjambs' 'equivocating']\n",
      "['kostić' 'smoking' 'answers.com' 'zhlobin' 'sikdar']\n",
      "['canisteo' 'khaos' 'beistline' 'bicci' 'brookins']\n",
      "['niazi' 'lohia' 'arvin' 'chosin' \"l'union\"]\n"
     ]
    }
   ],
   "source": [
    "# For getting random 5 tokens from the vocabulary\n",
    "\n",
    "for _ in range(10) :\n",
    "    print(np.random.choice(vocab, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3546a69",
   "metadata": {},
   "source": [
    "**Cosine Similarity** and **Levenshtein Distance** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34fba4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8994895926845297\n"
     ]
    }
   ],
   "source": [
    "# Cosine similarity can be calculated using\n",
    "\n",
    "from scipy import spatial\n",
    "vector1 =[1, 1, 2, 2, 3]\n",
    "vector2 = [1, 3, 1, 2, 6]\n",
    "\n",
    "cosine_similarity = 1 - spatial.distance.cosine(vector1, vector2)\n",
    "print(cosine_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a41677",
   "metadata": {},
   "source": [
    "Let's calculate the cosine similarity for two words : *grass* and *trees*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6925f2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score between grass and trees :  0.70772266\n"
     ]
    }
   ],
   "source": [
    "# Using the gensim library\n",
    "print(\"Similarity score between grass and trees : \", model.similarity('grass', 'trees'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a11a139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity between grass and trees :  0.707722544670105\n"
     ]
    }
   ],
   "source": [
    "# Using scipy\n",
    "cosine_similarity = 1 - spatial.distance.cosine(model['grass'], model['trees'])\n",
    "print(\"Cosine Similarity between grass and trees : \", cosine_similarity)\n",
    "\n",
    "# In the above code, model['grass'] and model['trees'] will give you the vector for those words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dadad53",
   "metadata": {},
   "source": [
    "* **LevenshTein Distance or Edit Distance:**\n",
    "    * Similarity between two words can be measured using this method.\n",
    "    * It is the **minimum number of single character edits** (insertion, deletion or substitution) required to **change one word into another**\n",
    "    * `levenshtein.distance('word1', 'word2')` is used to measure the edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c80feb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To install\n",
    "# !pip install levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49ce8b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a84323c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance('test', 'test') : 0 because no character substitution is needed\n",
      "distance('test', 'team') : 2 because two character substitution is needed; s -> a ; t -> m\n"
     ]
    }
   ],
   "source": [
    "print(f\"distance('test', 'test') : {distance('test', 'test')} because no character substitution is needed\")\n",
    "print(f\"distance('test', 'team') : {distance('test', 'team')} because two character substitution is needed; s -> a ; t -> m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6f2a86",
   "metadata": {},
   "source": [
    "**Short Comoigs of Word Embeddings :**\n",
    "* Since the models are dependent on the data they were trained on, they carry 2 significant side effects\n",
    "    1. Cultural Bias\n",
    "    2. Out-Of-Vocabulary ( OOV ) issues\n",
    "  \n",
    "**1. Cultural Bias:**\n",
    "* Model was trained on a massive US Google News Corpus. It learned the relationship between words on the news as seen by Google in US. There's nothing inherently biased about the news in the US versus some other corpus from another part of world, but training on such a dataset that the **model inherits a certain dose of cultural bias**.\n",
    "* For more critical issues, you should be aware that these models are **not universal or neutral but directly influenced by the corpus they were trained on**\n",
    "    \n",
    "**2. Out-Of-Vocabulary :**\n",
    "* GloVe and word2Vec models are finite nature in the model's vocabulary.\n",
    "* For words, where model fails to identify and give their vector representation, suck words are known as **OOV words**.\n",
    "* One way of handling the OOV words is by returning a **vector filled with zeros**.\n",
    "* Another way is updating the model by training the model with your own dataset. By this, the OOV words will end up with their own vector representation. This process is called **Fine-Tuning** or **Transfer Learning**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd606244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. cultural bias :\n",
    "# In the US, Alexis is a feminin name, while in the rest of the world it's a masculin name.\n",
    "# Word2vec was trained on US centric data. \n",
    "# This shows up when looking at the names the model condsiders \n",
    "# most similar to 'Alexis': Nicole, Erica, Marissa, Alicia ... all women names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8c6395a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['covidien']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# 2. OOV\n",
    "# Some words are not in Word2vec vocab's. for instance Covid and ... word2vec.\n",
    "\n",
    "vocab = model.index_to_key\n",
    "\n",
    "# no covid words (only 'covidien' which is a company)\n",
    "start_with = 'covid'\n",
    "vocab_subset = [tk.lower() for tk in  vocab if tk.lower()[:len(start_with)] == start_with]\n",
    "vocab_subset.sort()\n",
    "print(vocab_subset)\n",
    "\n",
    "# no word2vec words\n",
    "start_with = 'word2vec'\n",
    "vocab_subset = [tk.lower() for tk in  vocab if tk.lower()[:len(start_with)] == start_with]\n",
    "vocab_subset.sort()\n",
    "print(vocab_subset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
