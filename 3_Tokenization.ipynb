{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "501baeb5",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d1ea74",
   "metadata": {},
   "source": [
    "Using `NLTK` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62d3f177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let', \"'\", 's', 'eat', 'your', 'soup', ',', 'Grandpa', '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import tokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = WordPunctTokenizer().tokenize(\"Let's eat your soup, Grandpa.\")\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d0c2366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wikipedia_page(title):\n",
    "    '''\n",
    "    This function returns the raw text of a wikipedia page \n",
    "    given a wikipedia page title\n",
    "    '''\n",
    "    params = { \n",
    "        'action': 'query', \n",
    "        'format': 'json', # request json formatted content\n",
    "        'titles': title, # title of the wikipedia page\n",
    "        'prop': 'extracts', \n",
    "        'explaintext': True\n",
    "    }\n",
    "    # send a request to the wikipedia api \n",
    "    response = requests.get(\n",
    "         'https://en.wikipedia.org/w/api.php',\n",
    "         params= params\n",
    "     ).json()\n",
    "\n",
    "    # Parse the result\n",
    "    page = next(iter(response['query']['pages'].values()))\n",
    "    # return the page content \n",
    "    if 'extract' in page.keys():\n",
    "        return page['extract']\n",
    "    else:\n",
    "        return \"Page not found\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5553bbf",
   "metadata": {},
   "source": [
    "Let's apply tokenizer on the 'Earth' article from the wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fa9a84a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 743),\n",
       " (',', 589),\n",
       " ('.', 492),\n",
       " ('of', 364),\n",
       " ('and', 288),\n",
       " ('earth', 264),\n",
       " ('is', 174),\n",
       " ('to', 167),\n",
       " ('s', 160),\n",
       " (\"'\", 159),\n",
       " ('in', 157),\n",
       " ('a', 144),\n",
       " ('(', 110),\n",
       " ('-', 79),\n",
       " ('by', 77),\n",
       " ('as', 76),\n",
       " ('with', 75),\n",
       " ('from', 69),\n",
       " ('surface', 63),\n",
       " ('at', 59)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "import requests\n",
    "from collections import Counter\n",
    "\n",
    "# Extract the text from the page\n",
    "text = wikipedia_page('Earth').lower()\n",
    "\n",
    "# Apply the tokenizer to the text\n",
    "tokens = WordPunctTokenizer().tokenize(text)\n",
    "\n",
    "# Print the 20 most common tokens in text\n",
    "Counter(tokens).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09178a4a",
   "metadata": {},
   "source": [
    "* Tokenization on characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "198b6be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 9001),\n",
       " ('e', 5595),\n",
       " ('t', 4337),\n",
       " ('a', 4156),\n",
       " ('i', 3307),\n",
       " ('o', 3245),\n",
       " ('s', 3124),\n",
       " ('r', 3068),\n",
       " ('n', 3048),\n",
       " ('h', 2248),\n",
       " ('l', 2025),\n",
       " ('c', 1588),\n",
       " ('d', 1481),\n",
       " ('m', 1306),\n",
       " ('u', 1168),\n",
       " ('f', 949),\n",
       " ('p', 942),\n",
       " ('g', 843),\n",
       " ('y', 677),\n",
       " (',', 635)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exapmle for character tokenization\n",
    "char_tokens = [c for c in text]\n",
    "\n",
    "# Print the 20 most common character tokens\n",
    "Counter(char_tokens).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5d707c",
   "metadata": {},
   "source": [
    "* **N-Grams** Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a20fa7c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('How', 'much'),\n",
       " ('much', 'wood'),\n",
       " ('wood', 'would'),\n",
       " ('would', 'a'),\n",
       " ('a', 'woodchuck'),\n",
       " ('woodchuck', 'if'),\n",
       " ('if', 'a'),\n",
       " ('a', 'woodchuck'),\n",
       " ('woodchuck', 'could'),\n",
       " ('could', 'chuck'),\n",
       " ('chuck', 'wood'),\n",
       " ('wood', '?')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "text = \"How much wood would a woodchuck if a woodchuck could chuck wood?\"\n",
    "\n",
    "# Tokenize - normal word tokinze\n",
    "tokens = WordPunctTokenizer().tokenize(text)\n",
    "\n",
    "# Only keep the bigrams\n",
    "bigrams = [w for w in ngrams(tokens, n=2)]\n",
    "\n",
    "bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe28cb99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('How', 'much', 'wood'),\n",
       " ('much', 'wood', 'would'),\n",
       " ('wood', 'would', 'a'),\n",
       " ('would', 'a', 'woodchuck'),\n",
       " ('a', 'woodchuck', 'if'),\n",
       " ('woodchuck', 'if', 'a'),\n",
       " ('if', 'a', 'woodchuck'),\n",
       " ('a', 'woodchuck', 'could'),\n",
       " ('woodchuck', 'could', 'chuck'),\n",
       " ('could', 'chuck', 'wood'),\n",
       " ('chuck', 'wood', '?')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only trigrams\n",
    "trigrams = [w for w in ngrams(tokens, n=3)]\n",
    "\n",
    "trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f61438a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How_much',\n",
       " 'much_wood',\n",
       " 'wood_would',\n",
       " 'would_a',\n",
       " 'a_woodchuck',\n",
       " 'woodchuck_if',\n",
       " 'if_a',\n",
       " 'a_woodchuck',\n",
       " 'woodchuck_could',\n",
       " 'could_chuck',\n",
       " 'chuck_wood',\n",
       " 'wood_?']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join the bigrams to make a single word\n",
    "bi_tokens = ['_'.join(w) for w in bigrams]\n",
    "\n",
    "bi_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ecd9823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How_much_wood',\n",
       " 'much_wood_would',\n",
       " 'wood_would_a',\n",
       " 'would_a_woodchuck',\n",
       " 'a_woodchuck_if',\n",
       " 'woodchuck_if_a',\n",
       " 'if_a_woodchuck',\n",
       " 'a_woodchuck_could',\n",
       " 'woodchuck_could_chuck',\n",
       " 'could_chuck_wood',\n",
       " 'chuck_wood_?']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similarly join the trigrams\n",
    "tri_tokens = ['_'.join(w) for w in trigrams]\n",
    "\n",
    "tri_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50cee01",
   "metadata": {},
   "source": [
    "* Gloabl Vocabulary Size is very important for the processing speed and memory.\n",
    "* To reduce the size, **remove the all the words that are too rare or too frequent**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
