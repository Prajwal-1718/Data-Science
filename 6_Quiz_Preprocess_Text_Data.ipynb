{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb710883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# download the text\n",
    "result = requests.get('http://www.gutenberg.org/files/36/36-0.txt')\n",
    "\n",
    "# This line removes the header and footer\n",
    "text = result.text[840:].split(\"*** END\")[0]\n",
    "\n",
    "# This line removes the (weird) non ascii characters\n",
    "text = text.encode('ascii',errors='ignore').decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f54320a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\prajw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d79c31a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10046 unique lowercase tokens\n"
     ]
    }
   ],
   "source": [
    "#1. \n",
    "tokens_low = [tk.lower() for tk in text.split() ]\n",
    "print(f\"{len(set(tokens_low))} unique lowercase tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b523d8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters :  274677\n",
      "Number of unique_chars :  70\n",
      "Number of unique lowercase characters :  44\n"
     ]
    }
   ],
   "source": [
    "# 2.\n",
    "def chartokenizer(word):\n",
    "    # extracts all the characters in a given word\n",
    "    return [c for c in word]\n",
    "\n",
    "tokens = [txt for txt in text.split()]\n",
    "\n",
    "chars = []\n",
    "for tk in tokens :\n",
    "    chars += chartokenizer(tk)\n",
    "    \n",
    "lowercase_chars = [c.lower() for c in chars]\n",
    "print('Total number of characters : ', len(chars))\n",
    "\n",
    "print(\"Number of unique_chars : \", len(set(chars)))\n",
    "\n",
    "print(\"Number of unique lowercase characters : \", len(set(lowercase_chars)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4362bd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from collections import Counter\n",
    "# instantiate the tokenizer and tokenize in one line\n",
    "tokens = WordPunctTokenizer().tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f2cff72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 most common tokens:\n",
      " [('the', 4399), (',', 4134), ('.', 3141), ('and', 2358), ('of', 2284), ('a', 1529), ('I', 1264), ('to', 1157), ('in', 920), ('was', 850), ('that', 739), ('had', 565), ('it', 483), ('with', 435), ('my', 411), ('as', 402), ('at', 369), ('were', 368), ('on', 360), ('The', 346)]\n",
      "\n",
      "- tokens_count['the'] + tokens_count['The'] = 4745 which is less than 5000 \n",
      "68580\n"
     ]
    }
   ],
   "source": [
    "tokens_count = Counter(tokens)\n",
    "print(\"20 most common tokens:\\n\",tokens_count.most_common(20))\n",
    "print(f\"\\n- tokens_count['the'] + tokens_count['The'] = {tokens_count['the'] + tokens_count['The'] } which is less than 5000 \")\n",
    "total_token_count = len(tokens)\n",
    "print(total_token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f2c1cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22036\n",
      "Top 10 most common words make up  32.13% of total words\n"
     ]
    }
   ],
   "source": [
    "top_10_count = 0\n",
    "\n",
    "# token_count is a tuple: (token, count)\n",
    "for token_count in tokens_count.most_common(10):\n",
    "    top_10_count += token_count[1]\n",
    "print(top_10_count)\n",
    "\n",
    "print(f\"Top 10 most common words make up {top_10_count/total_token_count*100 : .2f}% of total words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9af5a6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e94ae417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 4134),\n",
       " ('.', 3141),\n",
       " ('-', 321),\n",
       " (';', 243),\n",
       " ('!', 202),\n",
       " ('one', 196),\n",
       " ('upon', 171),\n",
       " ('martians', 167),\n",
       " ('said', 166),\n",
       " ('people', 158),\n",
       " ('came', 150),\n",
       " ('saw', 131),\n",
       " ('towards', 129),\n",
       " ('black', 122),\n",
       " ('?', 121),\n",
       " ('time', 120),\n",
       " ('man', 119),\n",
       " ('could', 116),\n",
       " ('little', 112),\n",
       " ('road', 104)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [tk.lower() for tk in tokens if tk.lower() not in stopwords]\n",
    "Counter(tokens).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05094fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " 'Wells',\n",
       " '\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n   ',\n",
       " 'shall',\n",
       " 'dwell',\n",
       " 'world',\n",
       " 'inhabit',\n",
       " '\\r\\n    ',\n",
       " 'Lords',\n",
       " 'World']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5.\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(text)\n",
    "lemmas = [token.lemma_ for token in doc if not (token.is_stop or token.is_punct)]\n",
    "lemmas[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01038ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\r\\n', 4376),\n",
       " ('\\r\\n\\r\\n', 853),\n",
       " ('Martians', 163),\n",
       " ('London', 56),\n",
       " ('Woking', 50),\n",
       " ('Martian', 48),\n",
       " ('Mars', 42),\n",
       " ('Heat', 36),\n",
       " ('Ray', 34),\n",
       " ('\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n', 31),\n",
       " ('\\r\\n\\r\\n\\r\\n', 30),\n",
       " ('Hill', 29),\n",
       " ('\\r\\n ', 25),\n",
       " ('Weybridge', 24),\n",
       " ('Horsell', 23),\n",
       " ('Street', 23),\n",
       " ('Ogilvy', 20),\n",
       " ('Sunday', 20),\n",
       " ('Thames', 20),\n",
       " ('God', 19)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capitalized = [tk for tk in lemmas if tk.capitalize() == tk]\n",
    "Counter(capitalized).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f09619c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.\n",
    "persons = [ent.text for ent in doc.ents if ent.label_ in ['PERSON']]\n",
    "gpes = [ent.text for ent in doc.ents if ent.label_ in ['GPE']]\n",
    "locs = [ent.text for ent in doc.ents if ent.label_ in ['LOC']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9811afc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Londonward', 10),\n",
       " ('Ottershaw', 7),\n",
       " ('Chertsey', 7),\n",
       " ('Surrey', 7),\n",
       " ('Stent', 6),\n",
       " ('Heat-Ray', 6),\n",
       " ('Walton', 6),\n",
       " ('Weybridge', 5),\n",
       " ('Waterloo', 4),\n",
       " ('Ripley', 4),\n",
       " ('gaunt', 4),\n",
       " ('Putney Hill', 4),\n",
       " ('bush', 3),\n",
       " ('Kew', 3),\n",
       " ('Ditton', 3),\n",
       " ('George', 3),\n",
       " ('Elphinstone', 3),\n",
       " ('Miss Elphinstone', 3),\n",
       " ('Sheen', 3),\n",
       " ('Henderson', 2)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(persons).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "660ad2ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('London', 56),\n",
       " ('Woking', 23),\n",
       " ('Richmond', 11),\n",
       " ('Henderson', 10),\n",
       " ('Byfleet', 9),\n",
       " ('LONDON', 6),\n",
       " ('Shepperton', 6),\n",
       " ('Halliford', 6),\n",
       " ('England', 4),\n",
       " ('Pyrford', 4),\n",
       " ('Sunbury', 4),\n",
       " ('Maybury', 3),\n",
       " ('Titan', 3),\n",
       " ('Strand', 3),\n",
       " ('Kilburn', 3),\n",
       " ('Ripley', 3),\n",
       " ('Edgware', 3),\n",
       " ('Stanmore', 3),\n",
       " ('Ulla', 3),\n",
       " ('Cardigan', 2)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(gpes).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b13889de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mars', 42),\n",
       " ('Regents Park', 5),\n",
       " ('Venus', 4),\n",
       " ('EARTH', 2),\n",
       " ('Send', 2),\n",
       " ('New Barnet', 2),\n",
       " ('Sparks', 1),\n",
       " ('Smiths', 1),\n",
       " ('Earth', 1),\n",
       " ('the South-Eastern', 1),\n",
       " ('West Surrey', 1),\n",
       " ('Regent Street', 1),\n",
       " ('the Thames Valley', 1),\n",
       " ('Richmond Hill', 1),\n",
       " ('Richmond Park', 1),\n",
       " ('East Barnet', 1),\n",
       " ('Asia', 1),\n",
       " ('the Black Smoke', 1),\n",
       " ('Kensington Gardens', 1),\n",
       " ('Hyde Park', 1)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(locs).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5687da7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('come', '\\r\\n'), 41),\n",
       " (('heat', 'ray'), 37),\n",
       " (('house', '\\r\\n'), 37),\n",
       " (('see', '\\r\\n'), 36),\n",
       " (('\\r\\n', 'martians'), 33),\n",
       " (('\\r\\n', 'man'), 33),\n",
       " (('martians', '\\r\\n'), 32),\n",
       " (('man', '\\r\\n'), 30),\n",
       " (('\\r\\n', 'see'), 30),\n",
       " (('go', '\\r\\n'), 29),\n",
       " (('\\r\\n', 'people'), 28),\n",
       " (('\\r\\n', 'come'), 26),\n",
       " (('\\r\\n', 'house'), 25),\n",
       " (('red', 'weed'), 25),\n",
       " (('say', '\\r\\n\\r\\n'), 24),\n",
       " (('time', '\\r\\n'), 24),\n",
       " (('\\r\\n', 'road'), 23),\n",
       " (('people', '\\r\\n'), 23),\n",
       " (('thing', '\\r\\n'), 22),\n",
       " (('\\r\\n', '\\r\\n\\r\\n'), 22)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7.\n",
    "lemmas = [token.lemma_.lower() for token in doc if not (token.is_stop or token.is_punct)]\n",
    "\n",
    "from nltk.util import ngrams\n",
    "bigram_generator = ngrams(lemmas, n = 2)\n",
    "bigrams = [tk for tk in bigram_generator]\n",
    "Counter(bigrams).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46ee627e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('ulla', 'ulla', 'ulla'), 11),\n",
       " (('\\r\\n', 'heat', 'ray'), 8),\n",
       " (('heat', 'ray', '\\r\\n'), 8),\n",
       " (('black', 'smoke', '\\r\\n'), 7),\n",
       " (('red', 'weed', '\\r\\n'), 7),\n",
       " (('handling', 'machine', '\\r\\n'), 6),\n",
       " (('\\r\\n', 'sand', 'pit'), 5),\n",
       " (('\\r\\n', 'handling', 'machine'), 5),\n",
       " (('not', 'know', '\\r\\n'), 4),\n",
       " (('far', 'away', '\\r\\n'), 4),\n",
       " (('\\r\\n\\r\\n', 'come', 'say'), 4),\n",
       " (('see', '\\r\\n', 'martians'), 4),\n",
       " (('people', 'come', '\\r\\n'), 4),\n",
       " (('heat', 'ray', '\\r\\n\\r\\n'), 4),\n",
       " (('\\r\\n', 'fighting', 'machine'), 4),\n",
       " (('st.', 'georges', 'hill'), 4),\n",
       " (('st.', 'johns', 'wood'), 4),\n",
       " (('\\r\\n', 'strange', 'thing'), 3),\n",
       " (('hear', '\\r\\n', 'people'), 3),\n",
       " (('smoke', 'rise', '\\r\\n'), 3)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_generator = ngrams(lemmas, n = 3)\n",
    "trigrams = [tk for tk in trigram_generator]\n",
    "Counter(trigrams).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa81a183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' It was a sobbing alternation of\\r\\ntwo notes, Ulla, ulla, ulla, ulla, keeping on perpetually',\n",
       " '\\r\\n\\r\\nUlla, ulla, ulla, ulla, wailed that superhuman notegreat waves of\\r\\nsound sweeping down the broad, sunlit roadway, between the tall\\r\\nbuildings on each side',\n",
       " '\\r\\n\\r\\nUlla, ulla, ulla, ulla, cried the voice, coming, as it seemed to me,\\r\\nfrom the district about Regents Park',\n",
       " '\\r\\n\\r\\nI awoke to find that dismal howling still in my ears, Ulla, ulla,\\r\\nulla, ulla',\n",
       " ' That perpetual sound of Ulla,\\r\\nulla, ulla, ulla, confused my mind',\n",
       " ' As the yelping died away down the silent road, the\\r\\nwailing sound of Ulla, ulla, ulla, ulla, reasserted itself',\n",
       " '\\r\\n\\r\\nAs I crossed the bridge, the sound of Ulla, ulla, ulla, ulla, ceased']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8.\n",
    "sentences = text.split('.')\n",
    "[s for s in sentences if \"ulla\" in s.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27b2ced3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.180807117890055"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9.\n",
    "import numpy as np\n",
    "\n",
    "punctuation_signs = ['!', '(', ')', ',', '-', '.',':', ';', '?','_']\n",
    "\n",
    "# Splitting the text over '.', since we are considering the complete sentence\n",
    "sentences = text.split('.')\n",
    "\n",
    "# for each sentence add its length\n",
    "sentence_length = []\n",
    "for s in sentences:\n",
    "  tokens = [tk for tk in WordPunctTokenizer().tokenize(s) if tk not in punctuation_signs]\n",
    "  sentence_length.append(len(tokens))\n",
    "\n",
    "# average sentence length\n",
    "np.mean(sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae18784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "sentences = [sent.text.replace('\\r\\n', ' ').strip() for sent in doc.sents]\n",
    "\n",
    "sdoc = nlp(sentence)\n",
    "[token for token in sdoc if token.pos_ == 'ADJ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489e7333",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
